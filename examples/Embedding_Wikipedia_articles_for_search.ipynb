{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia記事の検索用埋め込み\n",
    "\n",
    "このノートブックは、[Question_answering_using_embeddings.ipynb](Question_answering_using_embeddings.ipynb)で使用される、Wikipediaの記事のデータセットを検索用に準備した方法を示しています。\n",
    "\n",
    "手順：\n",
    "\n",
    "0. 前提条件：ライブラリのインポート、APIキーの設定（必要な場合）\n",
    "1. 収集：2022年オリンピックに関する数百のWikipedia記事をダウンロードします\n",
    "2. チャンク：ドキュメントは短い、半自己完結のセクションに分割され、埋め込み対象となります\n",
    "3. 埋め込み：各セクションはOpenAI APIで埋め込まれます\n",
    "4. 保存：埋め込みデータはCSVファイルに保存されます（大規模なデータセットの場合、ベクトルデータベースを使用します）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 前提条件\n",
    "\n",
    "### ライブラリのインポート\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# インポート\n",
    "import mwclient  # サンプルのWikipedia記事をダウンロードするため\n",
    "import mwparserfromhell  # Wikipedia記事をセクションに分割するため\n",
    "import openai  # 埋め込みを生成するため\n",
    "import pandas as pd  # 記事セクションと埋め込みを格納するためのDataFrame\n",
    "import re  # Wikipedia記事から<ref>リンクを取り除くため\n",
    "import tiktoken  # トークンを数えるため\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ターミナルで `pip install` を使用して不足しているライブラリをインストールしてください。例：\n",
    "\n",
    "```zsh\n",
    "pip install openai\n",
    "```\n",
    "\n",
    "（ノートブックセルでも `!pip install openai` を使用できます。）\n",
    "\n",
    "ライブラリをインストールした場合は、ノートブックカーネルを再起動してください。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APIキーの設定（必要な場合）\n",
    "\n",
    "OpenAIライブラリは、`OPENAI_API_KEY`環境変数からAPIキーを読み込もうとします。まだ設定していない場合は、[こちらの手順](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety)に従ってこの環境変数を設定してください。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ドキュメントを収集する\n",
    "\n",
    "この例では、2022年冬季オリンピックに関連する数百のウィキペディア記事をダウンロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category:2022 Winter Olympics内の記事タイトルを 732 件見つけました。\n"
     ]
    }
   ],
   "source": [
    "# 2022年冬季オリンピックに関するWikipediaページを取得する\n",
    "\n",
    "CATEGORY_TITLE = \"Category:2022 Winter Olympics\"\n",
    "WIKI_SITE = \"en.wikipedia.org\"\n",
    "\n",
    "\n",
    "def titles_from_category(\n",
    "    category: mwclient.listing.Category, max_depth: int\n",
    ") -> set[str]:\n",
    "    \"\"\"指定したWikiカテゴリとそのサブカテゴリ内のページタイトルのセットを返す関数。\"\"\"\n",
    "    titles = set()\n",
    "    for cm in category.members():\n",
    "        if type(cm) == mwclient.page.Page:\n",
    "            # isinstance()の代わりにtype()を使用して、継承のないマッチをキャッチします\n",
    "            titles.add(cm.name)\n",
    "        elif isinstance(cm, mwclient.listing.Category) and max_depth > 0:\n",
    "            deeper_titles = titles_from_category(cm, max_depth=max_depth - 1)\n",
    "            titles.update(deeper_titles)\n",
    "    return titles\n",
    "\n",
    "\n",
    "site = mwclient.Site(WIKI_SITE)\n",
    "category_page = site.pages[CATEGORY_TITLE]\n",
    "titles = titles_from_category(category_page, max_depth=1)\n",
    "# max_depth=1は、カテゴリツリーを1レベル深く探索することを意味します\n",
    "print(f\"{CATEGORY_TITLE}内の記事タイトルを {len(titles)} 件見つけました。\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ドキュメントをチャンク化する\n",
    "\n",
    "参照ドキュメントが用意できたので、それを検索のために準備する必要があります。\n",
    "\n",
    "GPTは一度に読むことができるテキスト量に制限があるため、各ドキュメントを読むのに十分に短いチャンクに分割します。\n",
    "\n",
    "この具体的な例では、Wikipediaの記事に関して以下の作業を行います：\n",
    "- 外部リンクや脚注など、あまり関連性の低いセクションは削除します。\n",
    "- 参照タグ（例：<ref>）や空白、非常に短いセクションを削除してテキストをクリーンアップします。\n",
    "- 各記事をセクションに分割します。\n",
    "- 各セクションのテキストの前にタイトルとサブタイトルを追加し、GPTがコンテキストを理解するのを支援します。\n",
    "- セクションが長い場合（例：1,600トークン以上）、段落などの意味的な境界に沿って再帰的に小さなセクションに分割しようとします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Set\n",
    "import mwparserfromhell\n",
    "import mwclient\n",
    "\n",
    "\n",
    "SECTIONS_TO_IGNORE = [\n",
    "    \"See also\",\n",
    "    \"References\",\n",
    "    \"External links\",\n",
    "    \"Further reading\",\n",
    "    \"Footnotes\",\n",
    "    \"Bibliography\",\n",
    "    \"Sources\",\n",
    "    \"Citations\",\n",
    "    \"Literature\",\n",
    "    \"Footnotes\",\n",
    "    \"Notes and references\",\n",
    "    \"Photo gallery\",\n",
    "    \"Works cited\",\n",
    "    \"Photos\",\n",
    "    \"Gallery\",\n",
    "    \"Notes\",\n",
    "    \"References and sources\",\n",
    "    \"References and notes\",\n",
    "]\n",
    "\n",
    "\n",
    "def all_subsections_from_section(\n",
    "    section: mwparserfromhell.wikicode.Wikicode,\n",
    "    parent_titles: list[str],\n",
    "    sections_to_ignore: set[str],\n",
    ") -> list[tuple[list[str], str]]:\n",
    "    \"\"\"\n",
    "    Wikipediaのセクションから、すべてのネストされたセクションをフラット化したリストを返す関数。\n",
    "    各セクションはタプルであり、次のように構成されます：\n",
    "        - 最初の要素は親のサブタイトルのリストで、ページタイトルから始まります\n",
    "        - 2番目の要素はセクションのテキストです（子セクションは含まれません）\n",
    "    \"\"\"\n",
    "    headings = [str(h) for h in section.filter_headings()]\n",
    "    title = headings[0]\n",
    "    if title.strip(\"=\" + \" \") in sections_to_ignore:\n",
    "        # ^ウィキの見出しは \"== 見出し ==\" のようにラップされています\n",
    "        return []\n",
    "    titles = parent_titles + [title]\n",
    "    full_text = str(section)\n",
    "    section_text = full_text.split(title)[1]\n",
    "    if len(headings) == 1:\n",
    "        return [(titles, section_text)]\n",
    "    else:\n",
    "        first_subtitle = headings[1]\n",
    "        section_text = section_text.split(first_subtitle)[0]\n",
    "        results = [(titles, section_text)]\n",
    "        for subsection in section.get_sections(levels=[len(titles) + 1]):\n",
    "            results.extend(all_subsections_from_section(subsection, titles, sections_to_ignore))\n",
    "        return results\n",
    "\n",
    "def all_subsections_from_title(\n",
    "    title: str,\n",
    "    sections_to_ignore: set[str] = SECTIONS_TO_IGNORE,\n",
    "    site_name: str = WIKI_SITE,\n",
    ") -> list[tuple[list[str], str]]:\n",
    "    \"\"\"Wikipediaのページタイトルから、すべてのネストされたセクションをフラット化したリストを返す関数。\n",
    "    各セクションはタプルであり、次のように構成されます：\n",
    "        - 最初の要素は親のサブタイトルのリストで、ページタイトルから始まります\n",
    "        - 2番目の要素はセクションのテキストです（子セクションは含まれません）\n",
    "    \"\"\"\n",
    "    site = mwclient.Site(site_name)\n",
    "    page = site.pages[title]\n",
    "    text = page.text()\n",
    "    parsed_text = mwparserfromhell.parse(text)\n",
    "    headings = [str(h) for h in parsed_text.filter_headings()]\n",
    "    if headings:\n",
    "        summary_text = str(parsed_text).split(headings[0])[0]\n",
    "    else:\n",
    "        summary_text = str(parsed_text)\n",
    "    results = [([title], summary_text)]\n",
    "    for subsection in parsed_text.get_sections(levels=[2]):\n",
    "        results.extend(all_subsections_from_section(subsection, [title], sections_to_ignore))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2887c9ea5d8040d7b1b57eb203b137e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/732 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "732 ページで 5744 セクションが見つかりました。\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "# ページをセクションに分割\n",
    "# 100記事あたり約1分かかる場合があります\n",
    "wikipedia_sections = []\n",
    "for title in tqdm(titles):\n",
    "    wikipedia_sections.extend(all_subsections_from_title(title))\n",
    "print(f\"{len(titles)} ページで {len(wikipedia_sections)} セクションが見つかりました。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out 530 sections, leaving 5200 sections.\n"
     ]
    }
   ],
   "source": [
    "# テキストをクリーンアップする\n",
    "def clean_section(section: tuple[list[str], str]) -> tuple[list[str], str]:\n",
    "    \"\"\"\n",
    "    <ref>xyz</ref> パターンを削除し、先頭および末尾の空白を削除してクリーンなセクションを返します。\n",
    "    \"\"\"\n",
    "    titles, text = section\n",
    "    text = re.sub(r\"<ref.*?</ref>\", \"\", text)\n",
    "    text = text.strip()\n",
    "    return (titles, text)\n",
    "\n",
    "# 短い/空のセクションをフィルタリングする\n",
    "def keep_section(section: tuple[list[str], str]) -> bool:\n",
    "    \"\"\"セクションを保持すべき場合はTrueを返し、それ以外の場合はFalseを返します。\"\"\"\n",
    "    titles, text = section\n",
    "    if len(text) < 16:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "original_num_sections = len(wikipedia_sections)\n",
    "wikipedia_sections = [ws for ws in wikipedia_sections if keep_section(ws)]\n",
    "print(f\"{original_num_sections-len(wikipedia_sections)} 個のセクションをフィルタリングし、{len(wikipedia_sections)} 個のセクションが残りました。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lviv bid for the 2022 Winter Olympics']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{{Olympic bid|2022|Winter|\\n| Paralympics = yes\\n| logo = Lviv 2022 Winter Olym...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['Lviv bid for the 2022 Winter Olympics', '==History==']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[[Image:Lwów - Rynek 01.JPG|thumb|right|200px|View of Rynok Square in Lviv]]\\n...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['Lviv bid for the 2022 Winter Olympics', '==Venues==']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{{Location map+\\n|Ukraine\\n|border =\\n|caption = Venue areas\\n|float = left\\n|widt...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['Lviv bid for the 2022 Winter Olympics', '==Venues==', '===City zone===']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The main Olympic Park would be centered around the [[Arena Lviv]], hosting th...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['Lviv bid for the 2022 Winter Olympics', '==Venues==', '===Mountain zone===', '====Venue cluster Tysovets-Panasivka====']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'An existing military ski training facility in [[Tysovets, Skole Raion|Tysovet...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# print example data\n",
    "for ws in wikipedia_sections[:5]:\n",
    "    print(ws[0])\n",
    "    display(ws[1][:77] + \"...\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、長いセクションを再帰的により小さなセクションに分割します。\n",
    "\n",
    "テキストをセクションに分割するための完璧なレシピは存在しません。\n",
    "\n",
    "いくつかのトレードオフがあります：\n",
    "- より長いセクションは、より多くの文脈が必要な質問には向いているかもしれません\n",
    "- より長いセクションは、回収には不利かもしれません。なぜなら、複数のトピックが混在しているかもしれないからです\n",
    "- より短いセクションはコストを削減するために良いです（コストはトークンの数に比例しています）\n",
    "- より短いセクションは、より多くのセクションを回収することができ、これはリコールに役立つかもしれません\n",
    "- オーバーラップするセクションは、セクションの境界で回答が切れるのを防ぐのに役立つかもしれません\n",
    "\n",
    "ここでは、シンプルなアプローチを使用し、セクションを最大1,600トークンに制限し、長すぎるセクションを再帰的に半分に分割します。有用な文の途中で切断されるのを防ぐために、可能な限り段落の境界で分割します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Union\n",
    "\n",
    "GPT_MODEL = \"gpt-3.5-turbo\"  # トークナイザーを選択するためだけに重要\n",
    "\n",
    "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
    "    \"\"\"\n",
    "    文字列内のトークン数を返します。\n",
    "    \"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def halved_by_delimiter(string: str, delimiter: str = \"\\n\") -> List[str]:\n",
    "    \"\"\"\n",
    "    デリミタによって文字列を二つに分割し、各サイドのトークンをバランスよく保ちます。\n",
    "    \"\"\"\n",
    "    chunks = string.split(delimiter)\n",
    "    if len(chunks) == 1:\n",
    "        return [string, \"\"]  # デリミタが見つからない\n",
    "    elif len(chunks) == 2:\n",
    "        return chunks  # 中間点を探す必要はない\n",
    "    else:\n",
    "        total_tokens = num_tokens(string)\n",
    "        halfway = total_tokens // 2\n",
    "        best_diff = halfway\n",
    "        for i, _ in enumerate(chunks):\n",
    "            left = delimiter.join(chunks[: i + 1])\n",
    "            left_tokens = num_tokens(left)\n",
    "            diff = abs(halfway - left_tokens)\n",
    "            if diff >= best_diff:\n",
    "                break\n",
    "            else:\n",
    "                best_diff = diff\n",
    "        left = delimiter.join(chunks[:i])\n",
    "        right = delimiter.join(chunks[i:])\n",
    "        return [left, right]\n",
    "\n",
    "def truncated_string(\n",
    "    string: str, model: str, max_tokens: int, print_warning: bool = True\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    文字列を最大トークン数に制限して切り詰めます。\n",
    "    \"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    encoded_string = encoding.encode(string)\n",
    "    truncated_result = encoding.decode(encoded_string[:max_tokens])\n",
    "    if print_warning and len(encoded_string) > max_tokens:\n",
    "        print(f\"Warning: Truncated string from {len(encoded_string)} tokens to {max_tokens} tokens.\")\n",
    "    return truncated_result\n",
    "\n",
    "def split_strings_from_subsection(\n",
    "    subsection: Tuple[List[str], str],\n",
    "    max_tokens: int = 1000,\n",
    "    model: str = GPT_MODEL,\n",
    "    max_recursion: int = 5,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    サブセクションを最大トークン数に分割し、リストとして返します。\n",
    "    各サブセクションは親タイトル [H1, H2, ...] とテキスト (str) のタプルです。\n",
    "    \"\"\"\n",
    "    titles, text = subsection\n",
    "    full_string = \"\\n\\n\".join(titles + [text])\n",
    "    num_tokens_in_string = num_tokens(full_string)\n",
    "    \n",
    "    if num_tokens_in_string <= max_tokens:\n",
    "        return [full_string]\n",
    "    elif max_recursion == 0:\n",
    "        return [truncated_string(full_string, model=model, max_tokens=max_tokens)]\n",
    "    else:\n",
    "        for delimiter in [\"\\n\\n\", \"\\n\", \". \"]:\n",
    "            left, right = halved_by_delimiter(text, delimiter=delimiter)\n",
    "            if left == \"\" or right == \"\":\n",
    "                continue\n",
    "            else:\n",
    "                results = []\n",
    "                for half in [left, right]:\n",
    "                    half_subsection = (titles, half)\n",
    "                    half_strings = split_strings_from_subsection(\n",
    "                        half_subsection,\n",
    "                        max_tokens=max_tokens,\n",
    "                        model=model,\n",
    "                        max_recursion=max_recursion - 1,\n",
    "                    )\n",
    "                    results.extend(half_strings)\n",
    "                return results\n",
    "    return [truncated_string(full_string, model=model, max_tokens=max_tokens)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5200 Wikipedia sections split into 6059 strings.\n"
     ]
    }
   ],
   "source": [
    "# セクションをチャンクに分割\n",
    "MAX_TOKENS = 1600\n",
    "wikipedia_strings = []\n",
    "for section in wikipedia_sections:\n",
    "    wikipedia_strings.extend(split_strings_from_subsection(section, max_tokens=MAX_TOKENS))\n",
    "\n",
    "print(f\"{len(wikipedia_sections)}のWikipediaセクションを{len(wikipedia_strings)}の文字列に分割しました。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lviv bid for the 2022 Winter Olympics\n",
      "\n",
      "==History==\n",
      "\n",
      "[[Image:Lwów - Rynek 01.JPG|thumb|right|200px|View of Rynok Square in Lviv]]\n",
      "\n",
      "On 27 May 2010, [[President of Ukraine]] [[Viktor Yanukovych]] stated during a visit to [[Lviv]] that Ukraine \"will start working on the official nomination of our country as the holder of the Winter Olympic Games in [[Carpathian Mountains|Carpathians]]\".\n",
      "\n",
      "In September 2012, [[government of Ukraine]] approved a document about the technical-economic substantiation of the national project \"Olympic Hope 2022\". This was announced by Vladyslav Kaskiv, the head of Ukraine´s Derzhinvestproekt (State investment project). The organizers announced on their website venue plans featuring Lviv as the host city and location for the \"ice sport\" venues, [[Volovets]] (around {{convert|185|km|mi|abbr=on}} from Lviv) as venue for the [[Alpine skiing]] competitions and [[Tysovets, Skole Raion|Tysovets]] (around {{convert|130|km|mi|abbr=on}} from Lviv) as venue for all other \"snow sport\" competitions. By March 2013 no other preparations than the feasibility study had been approved.\n",
      "\n",
      "On 24 October 2013, session of the Lviv City Council adopted a resolution \"About submission to the International Olympic Committee for nomination of city to participate in the procedure for determining the host city of Olympic and Paralympic Winter Games in 2022\".\n",
      "\n",
      "On 5 November 2013, it was confirmed that Lviv was bidding to host the [[2022 Winter Olympics]]. Lviv would host the ice sport events, while the skiing events would be held in the [[Carpathian]] mountains. This was the first bid Ukraine had ever submitted for an Olympic Games.\n",
      "\n",
      "On 30 June 2014, the International Olympic Committee announced \"Lviv will turn its attention to an Olympic bid for 2026, and not continue with its application for 2022. The decision comes as a result of the present political and economic circumstances in Ukraine.\"\n",
      "\n",
      "Ukraine's Deputy Prime Minister Oleksandr Vilkul said that the Winter Games \"will be an impetus not just for promotion of sports and tourism in Ukraine, but a very important component in the economic development of Ukraine, the attraction of the investments, the creation of new jobs, opening Ukraine to the world, returning Ukrainians working abroad to their motherland.\"\n",
      "\n",
      "Lviv was one of the host cities of [[UEFA Euro 2012]].\n"
     ]
    }
   ],
   "source": [
    "# print example data\n",
    "print(wikipedia_strings[1])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ドキュメントチャンクの埋め込み\n",
    "\n",
    "ライブラリを短い自己完結型の文字列に分割したので、各文字列の埋め込みを計算できます。\n",
    "\n",
    "（大規模な埋め込みのジョブについては、[api_request_parallel_processor.py](api_request_parallel_processor.py)のようなスクリプトを使用して、レート制限を守りながらリクエストを並列化することができます。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 to 999\n",
      "Batch 1000 to 1999\n",
      "Batch 2000 to 2999\n",
      "Batch 3000 to 3999\n",
      "Batch 4000 to 4999\n",
      "Batch 5000 to 5999\n",
      "Batch 6000 to 6999\n"
     ]
    }
   ],
   "source": [
    "# 埋め込みを計算します\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"  # 2023年4月現在、OpenAIの最高の埋め込みモデル\n",
    "BATCH_SIZE = 1000  # 1リクエストあたり最大2048個の埋め込み入力を提供できます\n",
    "\n",
    "embeddings = []\n",
    "for batch_start in range(0, len(wikipedia_strings), BATCH_SIZE):\n",
    "    batch_end = batch_start + BATCH_SIZE\n",
    "    batch = wikipedia_strings[batch_start:batch_end]\n",
    "    print(f\"バッチ {batch_start} から {batch_end-1} の計算中\")\n",
    "    response = openai.Embedding.create(model=EMBEDDING_MODEL, input=batch)\n",
    "    for i, be in enumerate(response[\"data\"]):\n",
    "        assert i == be[\"index\"]  # 埋め込みが入力と同じ順序であることをダブルチェック\n",
    "    batch_embeddings = [e[\"embedding\"] for e in response[\"data\"]]\n",
    "    embeddings.extend(batch_embeddings)\n",
    "\n",
    "df = pd.DataFrame({\"text\": wikipedia_strings, \"embedding\": embeddings})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ドキュメントの断片と埋め込みを保存\n",
    "\n",
    "この例では、数千の文字列しか使用しないため、それらをCSVファイルに保存します。\n",
    "\n",
    "（より大きなデータセットの場合、パフォーマンスが向上するベクトルデータベースを使用してください。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save document chunks and embeddings\n",
    "\n",
    "SAVE_PATH = \"data/winter_olympics_2022.csv\"\n",
    "\n",
    "df.to_csv(SAVE_PATH, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
